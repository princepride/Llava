{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in d:\\anaconda\\lib\\site-packages (0.20.3)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\anaconda\\lib\\site-packages (from huggingface_hub) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\anaconda\\lib\\site-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda\\lib\\site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\anaconda\\lib\\site-packages (from huggingface_hub) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda\\lib\\site-packages (from huggingface_hub) (4.9.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests->huggingface_hub) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests->huggingface_hub) (2024.2.2)\n",
      "Downloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "   ---------------------------------------- 0.0/402.6 kB ? eta -:--:--\n",
      "   -------- ------------------------------- 81.9/402.6 kB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 402.6/402.6 kB 5.0 MB/s eta 0:00:00\n",
      "Installing collected packages: huggingface_hub\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.20.3\n",
      "    Uninstalling huggingface-hub-0.20.3:\n",
      "      Successfully uninstalled huggingface-hub-0.20.3\n",
      "Successfully installed huggingface_hub-0.23.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index-llms-huggingface 0.1.4 requires huggingface-hub<0.21.0,>=0.20.3, but you have huggingface-hub 0.23.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3480a7931c4705a75f5edb98787d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Llava\\openai\\clip-vit-large-patch14-336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\commands\\download.py:132: FutureWarning: Ignoring --local-dir-use-symlinks. Downloading to a local directory does not use symlinks anymore.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\n",
      "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]Downloading 'special_tokens_map.json' to 'openai\\clip-vit-large-patch14-336\\.huggingface\\download\\special_tokens_map.json.9bfb42aa97dcd61e89f279ccaee988bccb4fabae.incomplete'\n",
      "Downloading 'README.md' to 'openai\\clip-vit-large-patch14-336\\.huggingface\\download\\README.md.119dfc21c8e97d46a9be63cc69039ea5ef40d387.incomplete'\n",
      "Downloading 'config.json' to 'openai\\clip-vit-large-patch14-336\\.huggingface\\download\\config.json.e9c5a5dc325b017eda0b6e802f79283a1f68bf27.incomplete'\n",
      "Downloading '.gitattributes' to 'openai\\clip-vit-large-patch14-336\\.huggingface\\download\\.gitattributes.64f23e0770da589d2949e1c24149405f5eda3d68.incomplete'\n",
      "Downloading 'preprocessor_config.json' to 'openai\\clip-vit-large-patch14-336\\.huggingface\\download\\preprocessor_config.json.a4604d0a1564fa0c45a6ff087bada5066a6335ea.incomplete'\n",
      "Downloading 'merges.txt' to 'openai\\clip-vit-large-patch14-336\\.huggingface\\download\\merges.txt.bbfec752c9a675946c6dce106def6f35c882dcc2.incomplete'\n",
      "Downloading 'tf_model.h5' to 'openai\\clip-vit-large-patch14-336\\.huggingface\\download\\tf_model.h5.d12828ca8f0f3c92194f277b7d893da7f2fb7824d0b99dedb305eb48eb46bb7f.incomplete'\n",
      "Downloading 'pytorch_model.bin' to 'openai\\clip-vit-large-patch14-336\\.huggingface\\download\\pytorch_model.bin.c6032c2e0caae3dc2d4fba35535fa6307dbb49df59c7e182b1bc4b3329b81801.incomplete'\n",
      "Download complete. Moving file to openai\\clip-vit-large-patch14-336\\special_tokens_map.json\n",
      "Download complete. Moving file to openai\\clip-vit-large-patch14-336\\README.md\n",
      "Download complete. Moving file to openai\\clip-vit-large-patch14-336\\config.json\n",
      "Download complete. Moving file to openai\\clip-vit-large-patch14-336\\.gitattributes\n",
      "\n",
      "Fetching 11 files:   9%|▉         | 1/11 [00:00<00:09,  1.05it/s]Download complete. Moving file to openai\\clip-vit-large-patch14-336\\preprocessor_config.json\n",
      "Download complete. Moving file to openai\\clip-vit-large-patch14-336\\merges.txt\n",
      "Downloading 'tokenizer.json' to 'openai\\clip-vit-large-patch14-336\\.huggingface\\download\\tokenizer.json.564c0ebd5ce29c4ee4864004aee693deadd3128c.incomplete'\n",
      "Downloading 'tokenizer_config.json' to 'openai\\clip-vit-large-patch14-336\\.huggingface\\download\\tokenizer_config.json.24feaf785502565f990d348aafa501c254710ed8.incomplete'\n",
      "Downloading 'vocab.json' to 'openai\\clip-vit-large-patch14-336\\.huggingface\\download\\vocab.json.182766ce89b439768edadda342519f33802f5364.incomplete'\n",
      "Download complete. Moving file to openai\\clip-vit-large-patch14-336\\tokenizer_config.json\n",
      "Download complete. Moving file to openai\\clip-vit-large-patch14-336\\vocab.json\n",
      "Download complete. Moving file to openai\\clip-vit-large-patch14-336\\tokenizer.json\n",
      "Download complete. Moving file to openai\\clip-vit-large-patch14-336\\pytorch_model.bin\n",
      "\n",
      "Fetching 11 files:  55%|█████▍    | 6/11 [01:18<01:08, 13.65s/it]Download complete. Moving file to openai\\clip-vit-large-patch14-336\\tf_model.h5\n",
      "\n",
      "Fetching 11 files:  73%|███████▎  | 8/11 [01:20<00:28,  9.58s/it]\n",
      "Fetching 11 files: 100%|██████████| 11/11 [01:20<00:00,  7.36s/it]\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli download --resume-download openai/clip-vit-large-patch14-336 --local-dir openai/clip-vit-large-patch14-336 --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download --resume-download google/gemma-1.1-2b-it --local-dir google/gemma-1.1-2b-it --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoProcessor\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model_name_or_path = r\"D:\\Llava\\openai\\clip-vit-large-patch14-336\"\n",
    "gemma_model_name_or_path = r\"D:\\Llava\\meta-llama\\Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "d:\\Anaconda\\Lib\\site-packages\\transformers\\quantizers\\auto.py:159: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    }
   ],
   "source": [
    "clip_model = AutoModel.from_pretrained(clip_model_name_or_path)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(gemma_model_name_or_path,load_in_4bit = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "llm_tokenizer = AutoTokenizer.from_pretrained(gemma_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000, 128002]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_tokenizer.encode(\"<image>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlavaForConditionalGeneration, LlavaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_config = clip_model.vision_model.config\n",
    "text_config = llm_model.config\n",
    "configuration = LlavaConfig(vision_config, text_config)\n",
    "model = LlavaForConditionalGeneration(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = llm_tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.bos_token_id = llm_tokenizer.bos_token_id\n",
    "model.config.eos_token_id = llm_tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.image_token_index = llm_tokenizer.encode(\"<image>\")[0]\n",
    "model.config.image_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(r\"llava_model\\model002\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('llava_model\\\\model002\\\\tokenizer_config.json',\n",
       " 'llava_model\\\\model002\\\\special_tokens_map.json',\n",
       " 'llava_model\\\\model002\\\\tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_tokenizer.save_pretrained(r\"llava_model\\model002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoprocessor = AutoProcessor.from_pretrained(clip_model_name_or_path)\n",
    "autoprocessor.save_pretrained(r\"llava_model\\temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62cd69ee63e472eb7d4df5e63bea4c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration, LlavaProcessor, GemmaTokenizer, CLIPImageProcessor\n",
    "\n",
    "class MyLlavaProcessor(LlavaProcessor):\n",
    "    tokenizer_class = (\"LlamaTokenizer\", \"LlamaTokenizerFast\", \"GemmaTokenizer\")\n",
    "\n",
    "# model_id = \"test_model_copy/model001\"\n",
    "model_id = r\"llava_model\\model001\"  #\n",
    "\n",
    "\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ").to(0)\n",
    "\n",
    "processor = MyLlavaProcessor(image_processor=CLIPImageProcessor.from_pretrained(model_id), tokenizer=GemmaTokenizer.from_pretrained(model_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removed shared tensor {'language_model.lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4655800dbc4047358f31f7266da7a50c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921163c9865645e88a09678f770ca637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4994f5f9da455b804b3f7f14e633c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/642M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/princepride/init_gemma_clip_llava/commit/acaf708b16571cc39cc51d6fedd256078b370923', commit_message='Upload LlavaForConditionalGeneration', commit_description='', oid='acaf708b16571cc39cc51d6fedd256078b370923', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"init_gemma_clip_llava\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf824f1c3aa4a8a8b0ae1d124f5aa66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\Lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:573: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><image>\\nWhat are these?????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration, LlavaProcessor, GemmaTokenizer, CLIPImageProcessor\n",
    "\n",
    "class MyLlavaProcessor(LlavaProcessor):\n",
    "    tokenizer_class = (\"LlamaTokenizer\", \"LlamaTokenizerFast\", \"GemmaTokenizer\")\n",
    "\n",
    "# model_id = \"test_model_copy/model001\"\n",
    "model_id = r\"llava_model\\model001\"  #\n",
    "\n",
    "\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ").to(0)\n",
    "\n",
    "processor = MyLlavaProcessor(image_processor=CLIPImageProcessor.from_pretrained(model_id), tokenizer=GemmaTokenizer.from_pretrained(model_id))\n",
    "processor.tokenizer.add_bos_token = True\n",
    "prompt_text = r\"<image>\\nWhat are these?\"\n",
    "image_file = r\"000000039769.jpg\"\n",
    "raw_image = Image.open(image_file)\n",
    "inputs = processor(prompt_text, raw_image, return_tensors=\"pt\").to(0, torch.float16)\n",
    "\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n",
    "print(processor.decode(output[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     2,      7, 235286, 235254,   1841,    708,   1450, 235336]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'), 'pixel_values': tensor([[[[ 0.5435,  0.6455,  0.5581,  ...,  0.0909,  0.0033, -0.0696],\n",
       "          [ 0.5435,  0.6167,  0.5435,  ...,  0.1201,  0.0179,  0.0617],\n",
       "          [ 0.5581,  0.5581,  0.6602,  ...,  0.0909,  0.0764,  0.0617],\n",
       "          ...,\n",
       "          [ 1.8281,  1.8867,  1.8281,  ...,  1.4053,  1.4482,  1.5654],\n",
       "          [ 1.8574,  1.9014,  1.8721,  ...,  1.4775,  1.4053,  1.4922],\n",
       "          [ 1.8721,  1.9014,  1.9014,  ...,  1.4053,  1.2148,  1.4775]],\n",
       "\n",
       "         [[-1.3623, -1.2715, -1.3770,  ..., -1.4219, -1.4824, -1.5117],\n",
       "          [-1.3320, -1.2422, -1.3467,  ..., -1.4219, -1.4824, -1.4219],\n",
       "          [-1.2422, -1.2871, -1.1973,  ..., -1.4668, -1.4668, -1.4824],\n",
       "          ...,\n",
       "          [ 0.0789,  0.1239,  0.0338,  ..., -0.7168, -0.6567, -0.5664],\n",
       "          [ 0.1089,  0.1089,  0.0789,  ..., -0.6265, -0.7168, -0.6265],\n",
       "          [ 0.1239,  0.1089,  0.0789,  ..., -0.6416, -0.8818, -0.5513]],\n",
       "\n",
       "         [[-0.5562, -0.3853, -0.4138,  ..., -0.8687, -0.8545, -0.8687],\n",
       "          [-0.4563, -0.4421, -0.4849,  ..., -0.8120, -0.8828, -0.7832],\n",
       "          [-0.5273, -0.4421, -0.3994,  ..., -0.8687, -0.8262, -0.8403],\n",
       "          ...,\n",
       "          [ 1.6055,  1.5771,  1.5625,  ...,  0.8521,  0.7666,  0.8091],\n",
       "          [ 1.6055,  1.6621,  1.6621,  ...,  0.7808,  0.8662,  0.6670],\n",
       "          [ 1.6484,  1.6484,  1.6621,  ...,  0.8379,  0.8945,  0.8232]]]],\n",
       "       device='cuda:0', dtype=torch.float16)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.add_bos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.add_eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1efe093bd9994deb94de759e28de3cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(r\"D:\\Llava\\google\\gemma-1.1-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    r\"D:\\Llava\\google\\gemma-1.1-2b-it\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\Lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:573: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad><pad><pad><pad><pad><pad><pad><bos>Write me a poem about Machine Learning.\n",
      "\n",
      "In circuits of logic, a mind unseen,\n",
      "A tapestry of data, a world serene.\n",
      "Algorithms dance, a symphony of code,\n",
      "Learning from experience, a journey bold.\n",
      "\n",
      "From images and texts, to numbers and dates,\n",
      "The machine learns, a wondrous chase.\n",
      "Pattern recognition, a task of the past,\n",
      "Now automated, a future unsurpassed.\n",
      "\n",
      "In predictive modeling, forecasts so bold,\n",
      "Predicting future trends, stories untold.\n",
      "Clustering algorithms, sorting and grouping,\n",
      "Insights hidden, where knowledge is winning.\n",
      "\n",
      "From medical diagnosis to financial trends,\n",
      "Machine learning's impact, a guiding hand.\n",
      "It revolutionizes, a transformative force,\n",
      "Unlocking possibilities, forevermore.\n",
      "\n",
      "So let us embrace this technological might,\n",
      "Harnessing its power, shining bright.\n",
      "For in the realm of machines, a new dawn breaks,\n",
      "Where human and artificial, hand in graceful wakes.<eos>\n",
      "<bos>Help me calculate what's the answer of 1 + 1?\n",
      "\n",
      "The answer is 2.\n",
      "\n",
      "1 + 1 = 2<eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "input_text = [\"Write me a poem about Machine Learning.\", \"Help me calculate what's the answer of 1 + 1?\"]\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True)\n",
    "input_ids.to(\"cuda:0\")\n",
    "outputs = model.generate(**input_ids, max_length=512)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "print(tokenizer.decode(outputs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     0,      0,      0,      0,      0,      0,      0,      2,   5559,\n",
       "            682,    476,  19592,   1105,  13403,  14715, 235265],\n",
       "        [     2,  14795,    682,  13988,   1212, 235303, 235256,    573,   3448,\n",
       "            576, 235248, 235274,    963, 235248, 235274, 235336]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad><pad><pad><pad><pad><pad><pad><bos>Write me a poem about Machine Learning.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(input_ids.input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     2,   5559,    682,    476,  19592,   1105,  13403,  14715, 235265]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "In circuits of logic, a mind unseen,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[0],skip_special_tokens=True)[len(input_text):])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
